# Optional dependencies for local LLM serving (OpenAI-compatible endpoints)
vllm>=0.8.0
llama-cpp-python[server]>=0.2.90
